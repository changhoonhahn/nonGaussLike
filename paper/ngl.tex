\documentclass[12pt, letterpaper, preprint]{aastex}
\usepackage[breaklinks,colorlinks, urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{color}
\usepackage{amsmath}
\usepackage{bm}
\input{vc}

% typesetting shih
\linespread{1.08} % close to 10/13 spacing
\setlength{\parindent}{1.08\baselineskip} % Bringhurst
\setlength{\parskip}{0ex}
\let\oldbibliography\thebibliography % killin' me.
\renewcommand{\thebibliography}[1]{%
  \oldbibliography{#1}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parsep}{0pt}%
  \setlength{\parskip}{0pt}%
  \setlength{\bibsep}{0ex}
  \raggedright
}
\setlength{\footnotesep}{0ex} % seriously?

% citation alias
\defcitealias{beutler2017}{B2017}
\defcitealias{sinha2017a}{S2017}

% math shih
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\pseudo}{{\mathrm{pseudo}}}
\newcommand{\Var}{\mathrm{Var}}
% text shih
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\opcit}{\foreign{Op.~cit.}}
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\equationname}{equation}
\newcommand{\bitem}{\begin{itemize}}
\newcommand{\eitem}{\end{itemize}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\todo}[1]{{\bf \textcolor{red}{#1}}}
\newcommand{\Dmock}{\mathbf{D}^\mathrm{mock}}
\newcommand{\Xmock}{{\bf X}^\mathrm{mock}}
\newcommand{\Xref}{{\bf X}^\mathrm{ref}}
\newcommand{\Yref}{{\bf Y}^\mathrm{ref}}
\newcommand{\Ralpha}{R\'enyi-$\alpha$}
\newcommand{\Beut}{\citetalias{beutler2017}}
\newcommand{\Sinh}{\citetalias{sinha2017a}}

\newcommand{\patchy}{{\fontshape\scdefault\selectfont patchy}}

\begin{document}\sloppy\sloppypar\frenchspacing 

%\title{How I Learned to Stop Worrying and Love The Central Limit Theorem}
\title{Likelihood Non-Gaussianity in Large Scale Structure Analyses}
\date{\texttt{DRAFT~---~\githash~---~\gitdate~---~NOT READY FOR DISTRIBUTION}}
\author{ChangHoon~Hahn\refstepcounter{footnote}\refstepcounter{footnote}, et al.} %Florian~Beutler, Manodeep~Sinha, Andreas~Berlind}
\affil{Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, Berkeley CA 94720, USA}
\email{changhoon.hahn@lbl.gov}

\begin{abstract}
    abstract here 
\end{abstract}

\keywords{
methods: statistical
---
galaxies: statistics
---
methods: data analysis
---
cosmological parameters
---
cosmology: observations
---
large-scale structure of universe
}

\section{Introduction}
\begin{itemize}
    \item Talk about the use of Bayesian parameter inference and getting the posterior in LSS cosmology 
    \item Explain the two major assumptions that go into evaluating the likelihood
    \item Emphasize that we are not talking about non-Gaussian contributions to the likelihood
    \item Emphasize the scope of this paper is to address whether one of the assumptions matters for 
        galaxy clustering analyses. 
\end{itemize}

% Data analysis can only lead to unbiased constraints on a physical theory if the employed likelihood is correct (e.g.  Trotta 2008; Mardia et al. 1979; Anderson 2003; Cramer 1946; Jeffreys 1961; Sun & Berger 2006). 
\begin{itemize}
    \item Depending on Hogg's paper maybe a simple illustration of how the likelihood asumption 
\end{itemize}

\todo{However, as we show in this paper, the assumption of likelihood 
Gaussianity is not necessary. In fact, we will show that the mock catalogs 
used in standard LSS analyses to estimate the covariance matrix for 
evaluating the Gaussian likelihood, can be used to quantify the non-Gaussianity. 
More important the mock catalogs can be used to construct an accurate 
estimator for the non-Gaussian likelihood.} 

\section{Mock Catalogs}
Mock catalogs play an indispensable role in standard cosmological 
anslyses of LSS studies. They're used for testing analysis 
pipelines~\citep[][]{beutler2017, grieb2017, tinkerinpreparation}, 
testing the effect of systematics~\citep{guo2012, vargas-magana2014, hahn2017, pinol2017, ross2017}, 
and, most relevantly for this paper, estimating the covariance 
matrix~\citep[][]{parkinson2012, kazin2014, grieb2017, alam2017, beutler2017, sinha2017a}. 
In fact, nearly all current state-of-the-art LSS analyses use
covariance matrices estimated from mocks to evaluate the likelihood 
for parameter inference. 

While some argue for analytic estimates of the covariance 
matrix~\citep[e.g.][]{mohammed2017} or estimates directly from data
by subsampling~\citep[e.g.][]{norberg2009}, covariance matrices 
from mocks have a number of advantages. Mocks allow us 
to incorporate detailed systematic errors present in the 
data and variance beyond the volume of the data. Even for analytic 
estimates, a large ensemble of mocks are crucial for validataion~\citep[\emph{e.g.}][]{slepian2017}. 
Moreover, as we show later in this paper, mocks present an
additional advantage: they allow us to 
quantify the non-Gaussianity of the likelihood and more accurately 
estimate the true likelihood distribution. 

In this paper, we we focus on two LSS analyses: the powerspectrum 
multipole ($P_\ell$) analysis of \Beut~and group multiplicity 
function ($\zeta$) analysis of \Sinh. Throughout the paper we will 
make extensive use of the mock catalogs used in these analyses. 
In this section, we give a brief description of these mocks and how 
the observables used in the analysis --- the powerspectrum 
multipole ($P_\ell$) and group multiplicity function ($\zeta(N)$) ---
are calculated from them. Afterwards, we will describe how we compute 
the covariance matrix from the mocks and pre-process the mock observable
data.

\subsection{MultiDark-PATCHY~Mock Catalog} \label{sec:patchy}
In their powerspectrum multipole analysis, \Beut~use 
the MultiDark-\patchy~mock catalogs from \cite{kitaura2016}. 
These mocks are generated using the \patchy~code \citep{kitaura2014,kitaura2015}. 
They rely on large-scale density fields generated using augmented
Lagrangian Perturbation Theory~\citep[ALPT;][]{kitaura2013} on a mesh,
which they then populate with galaxies based on a combined non-linear
deterministic and stochastic biases. The mocks from the \patchy~code 
are then calibrated to reproduce the galaxy clustering in the 
high-fidelity BigMultiDark $N$-body simulation~\citep{rodriguez-torres2016, klypin2016}. 
Afterwards, the galaxies are assigned stellar masses using the 
{\fontshape\scdefault\selectfont hadron}~code~\citep{zhao2015}.
and the {\fontshape\scdefault\selectfont sugar}~code~\citep{rodriguez-torres2016} 
is applied to combine different boxes, incorporate selection
effects and masking to produce mock light-cone galaxy catalogs. 
The statistics of the resulting mocks are then compared to 
observations and the process is iterated to reach desired 
accuracy. We refer readers to~\cite{kitaura2016} for further 
details. 

In total, \cite{kitaura2016} generated a 12,228 mock light-cone 
galaxy catalogs for BOSS Data Release 12. In \Beut, 
they use 2045 and 2048 for the northern galactic cap (NGC) and 
souther galactic cap (SGC) of the LOWZ+CMASS combined sample. 
\Beut~excluded 3 mock realizations due to notable 
issues, which have been since been addressed. Therefore, 
in our analysis we use all 2048 mocks for both the NGC and SGC of 
the LOWZ+CMASS combined sample.

\subsection{\cite{sinha2017a} Mocks} \label{sec:gmf} 
The simulations used in the small-scale clustering analysis of \cite{sinha2017a} 
are from the Large Suite of Dark Matter Simulations project~\citep[LasDamas;][]{mcbride2009} 
designed to model galaxy samples from SDSS DR7. They use initial conditions derived from 
second order Lagrangian Perturbation Theory using the 2LTPIC code~\citep{scoccimarro1998, crocce2006}
and evolved them using the $N$-body $\mathtt{GADGET}$-$2$ code~\citep{springel2005}.
Halos are then identified from the dark matter distribution outputs using 
the $\mathtt{ntropy-fofsv}$ code~\citep{gardner2007}, which uses a 
friend-of-friends algorithm~\citep[FoF;][]{davis1985} with a linking length of $0.2$
times the mean inter-particle separation. %The FoF halo masses are then adjusted using the \cite{warren2006} correction. 
\Sinh~uses two configurations of the LasDamas simulations.
The Consuelo simulation contains $1400^3$ dark matter particles with 
mass of $1.87 \times 10^9\,h^{-1} M_\odot$ in a cubic volume of 
$420\,h^{-1} Mpc$ per side evolved from $z_\mathrm{init} = 99$. 
The Carmen simulation contains $1120^3$ dark matter particles with mass 
of $4.938 \times 10^{10}\,h^{-1} M_\odot$ in a cubic volume of 
$1000\,h^{-1} Mpc$ per side evolved from $z_\mathrm{init} = 49$. 
%They use the following cosmological parameters, which are motivated by the WMAP3 constraints \citep{spergel2007}: $\Omega_m = 0.25, \Omega_\Lambda = 0.75, \Omega_b = 0.04, h = 0.7, \sigma_8 = 0.8,\;\mathrm{and}\;n_s = 1.0.$

The FoF halo catalogs are then populated with galaxies using the 
`Halo Occupation Distribution` (HOD) framework. The 
number, positions, and velocities of galaxies are described statistically 
by an HOD model. \Sinh~adopts the `vanilla' HOD model of \cite{zheng2007}, 
where the mean number of central and satellite galaxies are described by 
the halo mass and five HOD parameters: $M_\mathrm{min}, 
\sigma_{\log\,M} , M_0, M_1,~\mathrm{and}~\alpha$. Lastly, once the 
simulation boxes are populated with galaxies, observational systematic 
effects are imposed. The peculiar velocities of galaxies are used to 
impose redshift-space distortions. Galaxies that lie outside the redshift
limits or sky footprint of the SDSS sample are removed. For further 
details regarding the mocks, we refer readers to \Sinh.

To calculate their covariance matrix, \Sinh~produced 200 
independent mock catalogs from 50 simulations using a single set of 
HOD model parameters. To take advantage of the methods we later 
present (Sections~\ref{sec:gmm} and~\ref{sec:ica}), we require a large 
number of mocks. Our methods rely on using the mocks to sample high 
dimensional distributions, so incorporating more mocks improves their 
accuracy. We utilize an addition $99$ sets of HOD parameters with $200$
mocks each. These parameters are sampled from the MCMC chain used to 
produce the posterior in~\Sinh. In total we use $20,000$ mocks. 

\subsection{Mock Observable ${\bf X}^\mathrm{mock}$ and Covariance Matrix $\mathbb{C}$} \label{sec:xmock}
To get from the mock catalogs described above to the covariance matrices 
used in~\Beut~and~\Sinh, the observables are measured for each mock in 
the \emph{same} way as the observations. We briefly describe how $P_\ell(k)$ and $\zeta(N)$ 
and their covariance matrices are measured in \Beut~and \Sinh. We then 
describe how we pre-process the mock observables for the methods we describe 
in the next sections.

To measure the powerspectrum multipoles of the BOSS DR12 galaxies
and the MutliDark-\patchy~mocks (Section~\ref{sec:patchy}), \Beut~uses a 
fast fourier transform (FFT) based anisotropic powerspectrum estimator 
based on \cite{bianchi2015} and \cite{scoccimarro2015}. This estimator 
estimates the  monopole, quadrupole, and hexadecapole 
($\ell = 0, 2,\,\mathrm{and}\,4$) of the powerspectrum using FFTs of the 
overdensity field multipoles for a given survey geometry. For further 
details on the estimator we refer readers to~\Beut~Section 3. \Beut~compute
the powerspectrum over the range $k = 0.01 - 0.15\, h\,\mathrm{Mpc}^{-1}$
for $\ell = 0,\,\mathrm{and}\,2$ and $k = 0.01 - 0.10\, h\,\mathrm{Mpc}^{-1}$.
for $\ell = 4$. The powerspectrum multipoles are calculated in bins of 
$\Delta k = 0.01\,h\,\mathrm{Mpc}^{-1}$.

From the $\vec{P}^{(n)} = \big[P^{(n)}_{0}(k), P^{(n)}_{2}(k), P^{(n)}_{4}(k) \big]$ 
of the MultiDark-\patchy~mocks, \Beut~computes the $(i, j)$ element of the 
covariance matrix of all multipoles as 
\beq
\mathbb{C}_{i,j} = \frac{1}{N_\mathrm{mock} - 1} \sum\limits_{n=1}^{N_\mathrm{mock}} \big[ \vec{P}^{(n)}_i - \overline{P}_i \big]
    \times \big[ \vec{P}^{(n)}_j - \overline{P}_j \big].
\eeq
$N_\mathrm{mock} = 2048$ is the number of mocks and $\overline{P}_i$ is the mean 
of the mock powerspectra: $\overline{P}_i = \frac{1}{N_\mathrm{mock}} \sum_{n=1}^{N_\mathrm{mock}} \vec{P}^{(n)}_i$.
Since $P_0$ and $P_2$ each have $14$ bins and $P_4$ has $9$ bins, 
$\mathbb{C}$ is a $37 \times 37$ matrix.
In this work, we compute the $P_\ell(k)$ using a similar FFT-based 
estimator of \cite{hand2017a} instead of the~\Beut~estimator. Our choice 
is based on computational convenience. A python implementation
of the \cite{hand2017a} estimator is publicly available in the $\mathtt{NBODYKIT}$ 
package\footnote{http://nbodykit.readthedocs.io/en/latest/index.html}. 
We confirm that the resulting $P_\ell(k)s$ and covariance matrices from 
the two estimators are consistent with one another. 

Next, for the \Sinh~group multiplicity function analysis, they start 
with the \cite{berlind2006} FoF algorithm to identify groups in the 
SDSS and mock data. \Sinh~adopts the \cite{berlind2006} linking lengths
in units of mean inter-galaxy separation: $b_\perp = 0.14$ and $b_\parallel = 0.75$.
In comoving lengths, the linking lengths for the SDSS DR7 $M_r < -19$ 
sample correspond to $(r_\perp, r_\parallel) = (0.57, 3.05)h^{-1}\,\mathrm{Mpc}$. 
Once the groups are identified in the SDSS and mock data, $\zeta(N)$
is derived by calculating the comoving number density of groups 
in bins of richness $N$ --- the number of galaxies in the group. 
For the $M_r < −19$ SDSS sample, \Sinh~uses eight $N$ bins: 
$(5 - 6), (7 - 9), (10 - 13), (14 - 19), (20 - 32), (33 - 52), (53 - 84), (85 - 220)$.
For further details on the GMF calculation, we refer readers to 
\Sinh~Section 4.2.  
From the $\zeta^{(n)}(N)s$ of each mock, \Sinh~computes the $(i, j)$ element of
the covariance matrix as 
\beq \label{eq:gmf_cov} 
\mathbb{C}_{i,j} = \frac{1}{N_\mathrm{mock} -1} \sum\limits_{n=1}^{N_\mathrm{mock}} \big[\zeta^{(n)}(N_i) -  \bar{\zeta}(N_i)\big] \times
    \big[\zeta^{(n)}(N_j) -  \bar{\zeta}(N_j)\big].
\eeq
In \Sinh, they compute the covariance matrix using $200$ mocks
generated using a single fiducial set of HOD parameters. As we 
describe in Section~\ref{sec:gmf}, in this paper we use $20,000$ mocks from 
$100$ different set of HOD parameters sampled from the MCMC chain. 
The GMF covariance matrix we use in this paper is computed with 
$N_\mathrm{mock} = 20,000$ mocks. 

For the rest of the paper, in order to discuss the two separate 
analyses of~\Beut~and~\Sinh~in a consistent manner, we define the matrix 
$\Dmock$ of the mock observables ($P_\ell$ and $\zeta$) as 
\beq
\Dmock = \Big\{{\bf D}^\mathrm{mock}_n \Big\} \quad\quad \mathrm{where}\,\,\Dmock_n
\begin{cases}
    \vec{P}^{(n)} &\quad\mathrm{for}\,\, \mathrm{B}2017,\\ 
    \zeta^{(n)} &\quad\mathrm{for}\,\, \mathrm{S}2017.
\end{cases} 
\eeq
$\Dmock$ has dimensions of $2048 \times 37$ and $20,000\times8$ 
for \Beut~and \Sinh~respectively. 

For the methods in Sections~\ref{sec:gmm}~and~\ref{sec:ica}, the mock 
observable data ($\Dmock$) need to be pre-processed. \todo{why preprocess}
This pre-processing
involves two steps: mean-subtraction and whitening. For mean subtraction, 
the mean of the observable is subtracted from $\Dmock$. Then 
$\Dmock - \overline{\bf D}^\mathrm{mock}$ is whitened using a linear transformation 
to remove the Gaussian correlation between the bins of $\Dmock$: 
\beq
\Xmock = L\,(\Dmock - \bar{\bf D}^\mathrm{mock}). 
\eeq
The linear transformation is derived so that covariance matrix of the whitened 
data, $\Xmock$, is the identity matrix $\mathbb{I}$. Such a whitening linear 
transformation can be derived in infinite ways. 
One way to derive the linear transformation is through the eigen-decomposition
of the covariance matrix~\citep[\emph{e.g.}][]{hartlap2009, sellentin2017}. We, alternatively, 
derive the linear transformation ${\bf L}$ using Choletsky decompositon of the 
inverse covariance matrix~\citep{Press:1992:NRC:148286}: 
$\mathbb{C}^{-1} = {\bf L}\,{\bf L}^T$. \todo{We confirm tha the different 
whitening algorithms, does not impact the results of the paper.}
Now that we have the pre-processed data of the mock observables, 
we proceed in the next section to quantifying the non-Gaussianity of the 
$P_\ell$ and $\zeta$ likelihoods. 


\section{Quantifying the Likelihood non-Gaussianity} \label{sec:div}
The standard approach to parameter inference in LSS studies neglects
to account for likelihood non-Gaussianity. 
However, we are not the first to investigate likelihood non-Gaussianity 
in LSS analyses. Nearly two decades ago, \cite{scoccimarro2000} examined 
the likelihood non-Gaussianity for the powerspectrum and reduced bispectrum 
using mock catalogs of the IRAS redshift catalogs. More recently, 
\cite{hartlap2009} and \cite{sellentin2017} examined the non-Gaussianity 
of the cosmic shear correlation function likelihood using simulations of 
the Chandra Deep Field South and CFHTLenS, respectively. 

While these works present different methods for identifying 
likelihood non-Gaussianity, they do not present a concrete way of 
quantifying it. \cite{hartlap2009}, for instance, identifies the
non-Gaussianity of the cosmic shear likelihood by looking at the
statistical independence/dependence of PCA components of the mock 
observable. In \cite{sellentin2017}, they use the Mean
Integrated Squared Error (MISE) as a distance metric between 
Gaussian random variables and the whitened mock observable 
data vector to characterize non-Gaussian correlations between elements 
of the data vector. These indirect measures of likelihood non-Gaussianity 
are challenging to interpret and extend more generally to LSS studies. 
%Sellentin & Heavens (2018) presented stringent tests that characterize non-Gaussian statistical behaviour, and any sound weak lensing likelihood should hence pass these tests.  The methods derived in Sellentin & Heavens (2018) test whether all pairwise combinations of data elements display the correct statistical behaviour under addition, division and multiplication, and whether the correct marginal distribution ensues for each data point. 

A more direct approach, however, can be taken to quantify the 
non-Gaussianity of the likelihood. We can calculate the divergence between 
the distribution of our observable, $p(x)$, and $q(x)$ a multivariate Gaussian described 
by the average of the mocks and the covariance matrix.
The following are two of the most commonly used divergences: 
the Kullback-Leibler (KL) divergence
\beq \label{eq:kl} 
D_{KL} ( p \parallel q ) = \int p(x)\,\log \frac{p(x)}{q(x)}\,{\rm d}x
\eeq
and the R\'enyi-$\alpha$ divergence
\beq \label{eq:renyi}
D_{R-\alpha} ( p \parallel q ) = \frac{1}{\alpha -1} \log \int p^\alpha(x)\, q^{1 -\alpha}(x)\,{\rm d}x. 
\eeq
In the limit as $\alpha$ approaches 1, the R\'enyi-$\alpha$ divergence is
equivalent to the KL divergence.
%To evaluate the R\'enyi-$\alpha$ divergence, we need to evaluate the following
%kernel function
%\beq \label{eq:d_alpha}
%D_{\alpha} ( p \parallel q ) = \int p^\alpha(x) q^{1-\alpha}(x)\,{\rm d}x. 
%\eeq

Of course, in our case, we don't know $p(x)$ --- \emph{i.e.} the distribution of
our observable. If we did, we would simply use that instead of bothering with 
the covariance matrix or this paper. We can, however, still estimate the 
divergence using nonparametric divergence estimators~\citep{wang2009, poczos2012, krishnamurthy2014}. 
These estimators allow us to estimate the divergence directly from 
samples $X_{1:n} = \{ X_1, ... X_n \}$ and $Y_{1:m} = \{ Y_1, ... Y_m \}$ 
drawn from $p$ and $q$ respectively: $\hat{D}_{\alpha}(X_{1:n} \parallel Y_{1:m})$. 
For instance, the estimator presented in \cite{poczos2012} allows us to estimate 
the kernel function of the R\'enyi-$\alpha$ divergence,
\beq \label{eq:d_alpha}
D_{\alpha} ( p \parallel q ) = \int p^\alpha(x) q^{1-\alpha}(x)\,{\rm d}x. 
\eeq
using $k^\mathrm{th}$ nearest neighbor density estimators. Let $\rho_k(x)$ 
denote the Euclidean distance of the $k^\mathrm{th}$ nearest neighbor 
of $x$ in the sample $X_{1:n}$ and $\nu_k(x)$ denote the Euclidean distance 
of the $k^\mathrm{th}$ nearest neighbor of $x$ in the sample $Y_{1:m}$. Then 
\beq \label{eq:d_alpha_est}
D_{\alpha}(p \parallel q) \approx \hat{D}_{\alpha}(X_{1:n} \parallel Y_{1:m}) = \frac{B_{k,\alpha}}{n} \left(\frac{n-1}{m}\right)^{1-\alpha}
\sum\limits_{i=1}^{n} \left(\frac{\rho_k^{d}(X_i)}{\nu_k^{d}(X_i)} \right)^{1-\alpha},
\eeq
where $B_{k, \alpha} = \frac{\Gamma(k)^2}{\Gamma(k-\alpha+1)\Gamma(k+\alpha-1)}$. 
This estimator, as \cite{poczos2012} proves, is asymptotically unbias,
\beq
\lim_{n, m \rightarrow \infty} \mathbb{E} \big[ \hat{D}_{\alpha} (X_{1:n} \parallel Y_{1:m}) \big] = D_{\alpha} (p \parallel q).
\eeq
Plugging $\hat{D}_{\alpha}(X_{1:n} \parallel Y_{1:m})$ into Eq.~\ref{eq:renyi},
we get an estimator for the R\'enyi-$\alpha$ divergence. \cite{wang2009} dervies
a similar estimator for the KL divergence~(Eq.~\ref{eq:kl}). 
%We note that while the divergence estimators converge to the true divergence with a large enough samples, with a limited number of samples from the distribution, the estimators are noisy. 

These divergence estimates have been applied to Support Vector Machines and used 
extensively in the machine learning and astronomical literature with great success 
\todo{elaborate a lot more} 
\bitem 
    \item Compile papers that use this divergence, \cite{ntampaka2015, ntampaka2016}
\eitem
For more details on the non-parametric divergence estimators, we refer readers to 
\cite{poczos2012} and \cite{krishnamurthy2014}.

Now we can use the divergence estimators above to quantify the 
non-Gaussianity of the likelihood. In other words, the divergence 
between the distribution $p(x)$ sampled by the mock observables 
$\Xmock$ (Section~\ref{sec:xmock}) and the multivariate Gaussian 
pseudo-likelihood distribution assumed in 
standard analyses, $\mathcal{N}(\overline{\bf X}^\mathrm{mock}, \mathbb{C})$. Since
$\Xmock$ is in principle sampled from $p(x)$, we draw a reference sample 
${\bf Y}^\mathrm{ref}$ from $\mathcal{N}(\overline{\bf X}^\mathrm{mock}, \mathbb{C})$ 
to use in the estimators. Similar to the experiments detailed in \cite{poczos2012}, 
we construct ${\bf Y}^\mathrm{ref}$ with a comparable sample size as $\Xmock$: 
$2000$ and $10,000$ for the $P_\ell$ and $\zeta$ analyses respectively. 

In Figure~\ref{fig:div_gauss}, we compare the distribution of 
\Ralpha~(left) and $KL$ (right) divergence estimates (orange)
$\hat{D}_{R\alpha}$ and $\hat{D}_{KL}$ between the mock data 
${\bf X}^\mathrm{mock}$ and a reference sample 
${\bf Y}^\mathrm{ref} \sim \mathcal{N}$ for the $P_\ell(k)$ (top) and $\zeta(N)$ (bottom) analyses.
These distributions were constructed using \todo{100} divergence estimates, 
where $\Yref$ is resampled for each estimate. In order to illustrate the scatter of the estimators and also  
as a reference point for the comparison, we include (blue) 
the distribution of $\hat{D}_{R\alpha}(\Xref \parallel \Yref)$ 
and $\hat{D}_{KL}(\Xref \parallel \Yref)$ where $\Xref$ is a data vector
with the same dimension as $\Xmock$ sampled from $\mathcal{N}(\overline{\bf X}^\mathrm{mock}, \mathbb{C})$.
%$\hat{D}_{R\alpha}(\Xmock \parallel \Yref)$ and $\hat{D}_{KL}(\Xmock \parallel \Yref)$ are \emph{estimates} of the actual divergence. 

The discrepancy between the blue and orange distributions illustrates the
non-Gaussianity of $p(x)$ sampled by ${\bf X}^\mathrm{mock}$. 
\bitem 
    \item Describe the discrepancy. 
\eitem

\section{Estimating the Non-Gaussian Likelihood}
In the previous section, we estimate the divergence between the 
$P_\ell$ and $\zeta$ likelihoods sampled by mocks and their respective 
Gaussian pseudo-likelihoods. The divergence estimates quantify 
the non-Gaussianity of the likelihoods and discredit the Gaussian 
likelihood assumption in standard LSS studies. It's not clear
how the divergences propagate onto the inferred parameter constraints. 
We're interested 
in quantifying the impact of likelihood non-Gaussianity on the final 
cosmological parameter constraints and also in developing more accurate 
methods for parameter inference in LSS analyses.
While the divergence estimates of the previous are useful for 
quantifying non-Gaussianity, In this section,
we present two methods for estimating the non-Gaussian likelihood 
distribution of $P_\ell$ and $\zeta$ from the corresponding mocks.
These methods will be used later to quantify the impact
of likelihood non-Gaussianity on the parameter constraints of 
\Beut~and \Sinh. Moreover, they provide a general framework for 
more accurately estimating the likelihood distribution than 
the Gaussian pseudo-likelihood. 

\subsection{Gaussian Mixture Likelihood Estimation} \label{sec:gmm}
When mock catalogs are used in LSS analyses to estimate the 
covariance matrix, they are used as data points sampling the 
likelihood distribution. For the pseudo-likelihood, 
this distribution is assumed to have a Gaussian functional form. 
However, the Gaussian functional form, or any functional form for 
that matter, is not necessary to estimate the likelihood distribution. 
Instead, the $N_\mathrm{bin}$-dimensional likelihood distribution 
can be directly estimated from the set of mock catalogs using --- for 
instance, Gaussian mixture density estimation~\citep{9780471006268}. 
\todo{talk about how this is used extensively in ML} 
In astronomy, Gaussian mixture density estimation has been used for 
inferring the velocity distribution of stars from the Hipparcos 
satellite~\citep{bovy2011}, classifying galaxies in the Galaxy And Mass Assembly 
Survey~\cite{taylor2015}, and classifying pulsars~(\cite{lee2012}; see
also references in \cite{kuhn2017}). 

Gaussian mixutre density estimation is a ``semi-parametric'' method 
that uses a weighted sum of $k$ Gaussian component densities (a Gaussian 
mixture model)
\beq
p({\bf x}; \bm{\theta}) = \sum\limits_{i=1}^{k} \pi_i\, \mathcal{N}({\bf x}; \bm{\theta}_i),
\eeq
to estimate the density. 
The component weights ($\pi_i$; also known as mixing weights) and the 
component parameters $\bm{\theta}_i$ are free parameters of the mixture 
model. Given some data set ${\bf X}_N = \{{\bf x}_1,..., {\bf x}_N \}$, 
the free parameters of the gaussian mixture model are, most popularly, estimated 
through an expectation-maximization algorithm~\citep[EM;][]{dempster1977, neal1998}.
The EM algorithm begins by randomly assigning $\bm{\theta}^0_i$ to the 
$k$ Gaussian componenets. The algorithm then iterates between two steps. 
In the first step, the algorithm computes, for each data point ${\bf x}_n$, 
a probability of being generated by each component of the model. These 
probabilities can be thought of as weighted assignments of the points 
to the components. Next, given the assignment of ${\bf x}_n$ to the 
components, $\bm{\theta}^t_i$ of each component are updated to $\bm{\theta}^{t+1}_i$
to maximize the likelihood of the assigned points. At this point, the mixing 
weights $\pi_i$ can also be updated by summing up the assignment weights 
and normalizing it by $N$, the total number of data points. This entire
process is repeated until convergence --- \emph{i.e.} when the log-likelihood of 
given the mixture model $\log\,\mathcal{L} = \log\,p({\bf X}_N; \bm{\theta}^t)$ %= \sum\limits_{n=1}^{N} \log\,p({\bf x}_n; \bm{\theta}^t)
converges. The EM algorithm is guaranteed to converges to a local maximum 
of the likelihood~\citep{wu1983}. 

In practice, instead of arbitrarily assigning the initial $\bm{\theta}^0_i$
more commonly $\bm{\theta}^0_i$ is derived from a $\mathtt{k-means}$ 
clustering algorithm~\citep{lloyd1982}. Without going into details, the 
$\mathtt{k-means}$ algorithm clusters the data ${\bf X}_N$ into $k$ 
clusters, each described by the mean (or centroid) $\mu_i$ of the
samples in the cluster. The algorithm then iteratively chooses centroids that 
minimize the average squared distance between points in the same cluster.
In our use of Gaussian mixture modelling, we set the initial conditions of 
the EM algorithm using the $\mathtt{k-means}{\tiny ++}$ algorithm of ~\cite{arthur2007}. 
In Figure~\ref{fig:gmf_ped}, we illustrate Gaussian Mixture 
density estimation in action. We use Gaussian mixture models with 
$k = 1$ (top), $3$ (middle), $10$ (bottom) components to estimate the
distribution of one dimensional data drawn from three separate Gaussia 
distributions. 
\todo{As an illustration of Gaussian mixture 
density estimation, in Figure we show Gaussian mixture models of the 
highest $N$ bin of $\Xmock$.}

So far in our discussion of Gaussian mixture modeling, we have kept
the number of componenets $k$ fixed. However, $k$ is a free 
parameter and selecting it is a crucial step in Gaussian mixture
density estimation. With too many components the model may overfit 
the data; with too few components, 
the model may not be flexible enough to approximate the true 
underlying distribution. In order to address this model selection problem
of selecting $k$, we make use of the Bayesian Information 
Criterion~\citep[BIC;][]{schwarz1978}. BIC has been widely used for 
determining the number of components in mixture 
modeling~\citep{leroux1992,roeder1997,fraley1998,steele2010performance}
and for model selection in general in 
astronomy~\citep[\emph{e.g.}][]{liddle2007,broderick2011,wilkinson2015,vakili2016}.
According to BIC, models with higher likelihood are preferred; however, 
to address the concern of overfitting, BIC introduces a penalty term 
for the number of parameters in the model: 
\beq \label{eq:bic}
\mathrm{BIC} = -2\,\mathrm{ln}\,\mathcal{L} + N_\mathrm{par}\,\mathrm{ln}N_\mathrm{data}.
\eeq
We select $k$ based on the number of components in the model with the 
lowest BIC. 

With the Gaussian mixture density estimation method described above
we can directly estimate the likelihood distribution using the mock catalogs. 
We first fit Gaussian mixture models with $k < 30$ components to the whitened 
mock data $\Xmock$ using the EM algorithm for each model. For each of the 
coverged Gaussian mixture models, we then calculate the BIC. Afterwards we select 
the model with the lowest BIC as the best density estimate of the likelihood 
distribution: $p_\mathrm{\tiny GMM}(\Xmock)$. The selected density estimate can 
then be used to calculate the
likelihood and quantify the impact of likelihood non-Gaussianity on the 
parameter constraints of~\Beut~and~\Sinh. But before we do that, we have to 
confirm whether $p_\mathrm{\tiny GMM}(\Xmock)$ is in fact an improved estimate 
of the likelihood over the Gaussian pseudo-likelihood. To do this, we return 
to the divergence estimates of Section~\ref{sec:div}. 

To estimate the divergence between our Gaussian mixture density estimate,
$p_\mathrm{\tiny GMM}(\Xmock)$, and the likelihood distribution, we take 
the same approach as our $D(\Xmock \parallel \Yref)$ calculation. We draw samples from 
$p_\mathrm{\tiny GMM}(\Xmock)$ with the same dimensions as $\Yref$. 
Then we calculate $k$-NN~\Ralpha~and KL divergence estimates (Section~\ref{sec:div}) 
between this sample and $\Xmock$. As we did in Figure~\ref{fig:div_gauss}, 
we repreat this process \todo{100} times, resampling $p_\mathrm{\tiny GMM}(\Xmock)$
each time, in order get a distribution of divergence estimates that 
reflects the scatter in the estimator. In Figure~\ref{fig:div_nongauss}, 
we present the resulting distribution of divergences between 
$p_\mathrm{\tiny GMM}(\Xmock)$ and the likelihood distribution in \todo{green} 
for the $P_\ell(k)$ (top) and $\zeta(N)$ (bottom) analyses. For comparison, 
we also include the distributions from Figure~\ref{fig:div_gauss}. 

For the $\zeta(N)$ analysis of \Sinh, our Gaussian mixture density 
estimate significantly improves the divergence discrepancy compared to the 
pseudo-likelihood. In other words, \emph{our Gaussian mixture density estimate  
is a significant better estimate of the $\zeta$ likelihood distribution
than the pseudo-likelihood}. On the other hand, our Gaussian mixture 
density estimate for the $P_\ell(k)$ analysis of \Beut~does not 
significantly improve the divergence discrepancy. The difference in 
the performance of Gaussian mixture density estimation is not a surprise. 
One would expect a direct density estimation to be more effective 
for the \Sinh~case, where we are estimating a $N_\mathrm{bin} = 8$ dimensional 
distribution with $N_\mathrm{mock} = 20,000$ samples, compared to the \Beut~ 
case. For \Beut~we are estimating a higher dimensional distribution 
($N_\mathrm{bin} = 37$) with fewer samples ($N_\mathrm{mock} = 2048$). 
Given the unconvincing accuracy of the Gaussian mixture density estimate
of the $P_\ell$ likelihood, in the next section, we present an alterative 
method for estimating the non-Gaussian likelihood.

\subsection{Independent Component Analysis} \label{sec:ica}
As Figure~\ref{fig:div_nongauss} reveals, Gaussian mixture density
estimation fails to accurately estimate the $37$-dimensional 
$P_\ell$ likelihood distribution of the \Beut~analysis. Rather than 
estimating the high dimensional likelihood distribution directly, 
following the approach from \cite{hartlap2009}, consider a linear
transformation on the observable ($P_\ell$) into $N_\mathrm{bin}$ 
statistically independent components. \todo{equation} 
Since each component is statistically 
independent from the rest, the likelihood becomes the multiple 
of $N_\mathrm{bin}$ one dimensional distributions. \todo{equation}
For the \Beut~case, this reduces the problem of estimating a 37 
dimensional distribution with $2048$ samples to a problem of 
estimating 37 one dimensional distributions with $2048$ samples 
each. The challenge, however, is in finding the transformation. 

Efforts in the past have similarly attempted to tackle the 
high-dimensionality problem of the observable space~\citep[\emph{e.g.}][]{scoccimarro2000,eisenstein2001,gaztanaga2005,norberg2009,sinha2017a}.
These works typically use singular value decomposition or principal component 
analysis~\citep[PCA;][]{Press:1992:NRC:148286}. For 
a Gaussian likelihood, the resulting components from such a 
decomposition/transformation are statistically independent.
However, when the likelihood is not Gaussian the PCA components 
are uncorrelated but~\emph{not necessarily statistically independent}~\citep{hartlap2009}. 
Therefore, instead of PCA, we use Independent Component Analysis 
(ICA). 

%An intuitive, though slightly hand-waving way to understand how ICA works is to note that a set of linear combinations Yi of independent, non-Gaussian random variables X j will usually have distributions that are more Gaussian than the original dis- tributions of the X j (central limit theorem). Reversing this argu- ment, this suggests that the X j could be recovered from a sample of the Yi by looking for linear combinations of the Yi that have the least Gaussian distributions. These linear combinations will also be close to statistically independent. A more rigorous justi- fication of the method can be found in Hyvärinen et al. (2001).
\bitem
    \item describe ICA 
    \item Some pedagogical figure that shows how ICA works. 
\eitem
\todo{describe ICA here} 

Using ICA, we decompose/transform $\Xmock$ with the unmixing matrix ${\bf W}$ 
into $N_\mathrm{bin}$ independent components: 
\beq
{\bf X}^\mathrm{ICA} = {\bf W}\,\Xmock = \{{\bf X}^\mathrm{ICA}_1, ..., {\bf X}^\mathrm{ICA}_{N_\mathrm{bin}}\}.
\eeq
The statistical indpendence of these components, allow us to write 
down the likelihood distribution as 
\beq \label{eq:ica_like}
\mathcal{L} \approx \prod\limits_{n=1}^{N_\mathrm{bin}} p_{x^\mathrm{ICA}_n} (x) 
\eeq
where $p_{x^\mathrm{ICA}_n} (x)$ is the 1-dimensional distribution 
function of the $n^\mathrm{th}$ ICA component, which in our context 
is sampled by ${\bf X}^\mathrm{ICA}_n$. In order to estimate $p_{x^\mathrm{ICA}_n}$s, 
we use the kerenl density method~\citep[KDE; \emph{e.g.}][]{9780387848587,feigelson2012}
following \cite{hartlap2009}. With KDE, the density estimate,
$\hat{p}_{x^\mathrm{ICA}_n}$, is constructed by 
smoothing the empirical distribution of the ICA componenet $x^\mathrm{ICA}_n$ 
using a smooth kernel. In our context, the KDE density estimate 
can be written as 
\beq
\hat{p}_{x^\mathrm{ICA}_n}(x) = \frac{1}{N_\mathrm{mock}b} \sum\limits_{j=1}^{N_\mathrm{mock}} K \left( \frac{x - \mathrm{X}^{(j),\mathrm{ICA}}_n}{b} \right)
\eeq
$b$ is the bandwidth and $K$ is the kernel function. Following the 
choices of \cite{hartlap2009}, we use a Gaussian distribution for $K$ and the 
``rule of thumb'' bandwidth~\cite[also known as Scott's rule;][]{scott1992,davison2008} 
for $b$. Finally, plugging $\hat{p}_{x^\mathrm{ICA}_n}$ into Eq.~\ref{eq:ica_like}, 
we get an estimate for the high-dimensional $\mathcal{L}_X$.

Now that we have a likeilhood estimate from ICA, we test whether it's
actually a better estimate of the likelihood distribuiton
than the Gaussian pseudo-likelihood. Following the same procedure as we 
did for the Gaussian mixture likelihood in Section~\ref{sec:gmm}, we 
calculate the divergence between our ICA likelihood, $\prod \hat{p}_{x^\mathrm{ICA}_n}$, 
and the likelihood distribution. We draw a sample from $\prod \hat{p}_{x^\mathrm{ICA}_n}$
with the same dimensions as $\Yref$, apply the mixing matrix 
(undoing the ICA transformation), and then calculate the 
$k$-NN~\Ralpha~and KL divergence estimates between the sample and $\Xmock$. 
We repeat these steps \todo{100} times to get the distribution of estimates 
that reflects the scatter in the estimator. In \todo{Figure~\ref{fig:div_nongauss}}, 
we present the resulting distribution of 
$\hat{D}\left(\Xmock \parallel \sim \prod \hat{p}_{x^\mathrm{ICA}_n} \right)$
in \todo{green} for the $P_\ell(k)$ (top) and $\zeta(N)$ (bottom) analyses. 
For comparison, we also include the distributions from Figure~\ref{fig:div_gauss}. 

In \emph{both} the \Beut~and~\Sinh~analyses, our ICA likelihood significantly 
improves the divergence discrepancy compared to the pseudo-likelihood. For 
\Sinh, however, the ICA likelihood proves to be a less accurate estimate
than the Gaussian mixture likelihood in Section~\ref{sec:gmm}. More importantly, 
for the~\Beut~analysis, where the Gaussian mixture likelihood did not improve 
upon the pseudo-likelihood, the ICA method provides a more accurate likelihood 
estimate. The ICA method provides an alterative to the more direct 
Gaussian mixture method. Furthermore, the effectiveness of the ICA method in estimating
higher dimensional likelihoods with fewer samples (mocks) is  
particularly appealing since LSS analyses continue to increase the size of 
their observable data vector. However, as the divergence estimation framework 
we present makes it easy to test the accuracy of different methods, a hard 
choice is not necessary and multiple methods can easily be tested to construct 
the best estimate of the likelihood distribution for each specific analysis. 
In the following sections, we use the ICA method for the~\Beut~analysis and 
the Gaussian mixture method for the~\Sinh~analysis. 

\section{Impact on Parameter Inference}
Both~\Beut~and~\Sinh~use the standard Monte Carlo Markov Chain (MCMC) 
approach with the Gaussian pseduo-likelihood to derive the posterior distributions
of their model parameters. The \Beut~analysis contains $11$ parameters:
\beq \nonumber
\Big \{f \sigma_8,~\alpha_\parallel,~\alpha_\perp,~b_1^\mathrm{NGC} \sigma_8,~b_1^\mathrm{SGC} \sigma_8, 
~b_2^\mathrm{NGC} \sigma_8,~b_2^\mathrm{SGC} \sigma_8,~\sigma_v^\mathrm{NGC},~\sigma_v^\mathrm{SGC}, 
~N^\mathrm{NGC},~\mathrm{and}~N^\mathrm{SGC} \Big \}. 
\eeq
%$f \sigma_8$, $\alpha_\parallel$, $\alpha_\perp$,
%$b_1^\mathrm{NGC} \sigma_8$, $b_1^\mathrm{SGC} \sigma_8$, 
%$b_2^\mathrm{NGC} \sigma_8$, $b_2^\mathrm{SGC} \sigma_8$, 
%$\sigma_v^\mathrm{NGC}$, $\sigma_v^\mathrm{SGC}$, 
%$N^\mathrm{NGC}$, and $N^\mathrm{SGC}$. 
Meanwhile the \Sinh~analysis contains $5$ parameters:
\beq \nonumber
\Big\{ \log\,M_\mathrm{min},~\sigma_{\log\,M},~\log\,M_0,~\log\,M_1,~\mathrm{and}~\alpha \Big\}. 
\eeq
%$\log\,M_\mathrm{min}$, $\sigma_{\log\,M}$, $\log\,M_0$, $\log\,M_1$, and $\alpha$. 
Using the improved likelihood estimates from the Sections~\ref{sec:gmm} 
and~\ref{sec:ica}, we can now measure the impact of likelihood non-Gaussianity 
on the posterior distributions of these parameters. The most straightforward 
approach to do this would be to use our likelihood estimates to recompute the 
MCMC samples from scratch. While this is relatively doable for the~\Beut~analysis, 
for~\Sinh this is \emph{significantly} more involved. Rather than a perturbation 
theory based model from~\Beut, the model in \Sinh~is a forward model, similar
to their mocks (Section~\ref{sec:gmf}). Rerunning the MCMC samples would involve
evaluating their computational intensive forward model $\sim 10^5$ times. 

Without having to rerun the MCMC chains, we instead use importance sampling 
to derive the new posteriors from the original chains~\citep[see][for details on importance sampling]{wasserman2004}. 
The \emph{target} distribution we want to sample is the new posterior. We can 
sample this distribution by using the original posterior as a \emph{proposal} 
distribution and the ratio of our likelihood estimates over the pseudo-likelihood 
as importance weights. If we let $p({\bf x} | \bm{\theta})$ be the original 
pseudo-likelihood and $p'({\bf x} | \bm{\theta})$ be our ``new'' likelihood,
then the new marginal likelihood can be calculated through importance sampling:   
\begin{align}
p'({\bf x} | \theta_1) &= \int p'({\bf x} | \bm{\theta})\,\mathrm{d}\theta_2...\mathrm{d}\theta_p = \int \frac{p'({\bf x} | \bm{\theta})}{p({\bf x} | \bm{\theta})}\, p({\bf x} | \bm{\theta})\,\mathrm{d}\theta_2...\mathrm{d}\theta_p \\
    &\approx \sum\limits_{\bm{\theta}^{(i)} \in S} \frac{p'({\bf x} | \bm{\theta}^{(i)})}{p({\bf x} | \bm{\theta}^{(i)})}. \label{eq:impsamp}
\end{align}
$S$ in Eq.~\ref{eq:impsamp} is the sample drawn from $p({\bf x} | \bm{\theta})$, 
which in our case is just the original MCMC chain. The only calculation
required is the importance weight 
${p'({\bf x} | \bm{\theta}^{(i)})}/{p({\bf x} | \bm{\theta}^{(i)})}$ for each 
sample $\bm{\theta}^{(i)}$ of the original MCMC chain. We calculate the 
importance weights for the~\Beut~and~\Sinh~analyses using the ICA likelihood 
and Gaussian mixture likelihood estimates respectively.  

In Figure~\ref{fig:pk_like}

In Figure~\ref{fig:gmf_like}

Figure~\ref{fig:rsd_contour}

Figure~\ref{fig:gmf_contour}

\section{Discussion}
\bitem
    \item Will it matter for future surveys? 
    \item Likelihood free inference (cite justin's paper) 
\eitem

\section{Summary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figures 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/kNNdiverg_Gauss.pdf}
\caption{R\'enyi-$\alpha$ and KL divergence estimates, ($D_{R\alpha}$ and $D_{KL}$), 
between the mock data ${\bf X}^\mathrm{mock}$ and a reference sample 
${\bf Y}^\mathrm{ref}$ for the $P_\ell(k)$ (left) and $\zeta(N)$ (right) analyses.}
\label{fig:div_gauss}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{figs/GMM_pedagog.pdf}
\caption{We use Gaussian mixture models with $k = 1$ (top), $3$, (middle), $10$ (bottom) 
    components to estimate the distribution of data (blue) drawn from three Gaussian distributions.
}
\label{fig:gmf_ped}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/kNNdiverg_gmm.pdf}
\caption{}
\label{fig:div_gmm}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/kNNdiverg_ica.pdf}
\caption{}
\label{fig:div_ica}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figs/Like_Pk_comparison.pdf}
\caption{}
\label{fig:pk_like}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/RSD_contours.pdf}
\caption{}
\label{fig:rsd_contour}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figs/Like_GMF_comparison.pdf}
\caption{}
\label{fig:gmf_like}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/GMFcontours_manodeep.pdf}
\caption{}
\label{fig:gmf_contour}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
It's a pleasure to thank 
    Simone~Ferraro,
    David~W.~Hogg,
    Emmaneul~Schaan, 
    Roman~Scoccimarro
    Zachary~Slepian

\bibliographystyle{yahapj}
\bibliography{nongausslike}
\end{document}
